# 黑客松项目质量评估方案

## 评估流程总览
- 建议采用“基础度量 → AI 智能评审 → 人工抽查”的三级流程：先由静态/覆盖率工具生成硬指标，再由 LLM 汇总证据、输出解释型评分，最后只对边缘案例人工复核。
- 统一质量模型可对照 ISO/IEC 25010:2023，将“功能适配性、性能效率、可维护性、安全性、兼容性、可移植性”等特性映射到评分表，避免单一维度失衡。
- 初始权重可设为“代码质量 40% + 测试成熟度 35% + 文档与可运维性 25%”，后续可结合赛况微调。

## 评分维度细则

### 1. 代码质量（40%）
- **静态分析基线**：要求提交 SonarQube 或等效报告，关注重复率、圈复杂度、技术债比、阻断级缺陷等；设置质量门槛（例如新代码零阻断缺陷、技术债比 ≤ 5%）。
- **结构化指标**：参考 SQALE/ISO 9126 派生维度（可维护性、可扩展性、可靠性等）折算 0–5 分；LLM 对照架构说明、代码目录检查分层是否清晰、设计模式应用情况，并提供证据路径。
- **性能与并发**：要求热点模块附基准测试或性能分析摘要；脚本验证是否包含 CPU/内存指标及数据来源，LLM 审核是否覆盖关键路径、资源上限说明。
- **安全与合规**：静态分析聚焦 OWASP Top 10 类缺陷；LLM 检查输入验证、鉴权拦截、敏感信息处理说明，如缺失则扣分。
- **架构合理性**：结合 ISO/IEC 25010 的兼容性与可移植性子项，LLM 核对架构图、README 与代码实现的一致性，标记不一致位置。

### 2. 测试成熟度（35%）
- **覆盖率门槛**：语句覆盖率 ≥ 80%，分支覆盖率 ≥ 60%；同步收集条件覆盖或 MC/DC 指标，防止“刷覆盖”。
- **Mutation Testing 加分**：鼓励提交 PIT 等变异测试得分，≥ 70% 可额外加分，体现断言质量。
- **测试工件完备性**：参考 ISO/IEC/IEEE 29119，检查测试计划、用例、缺陷报告、执行日志等；脚本先校验是否齐全，LLM 评估边界场景、异常路径覆盖情况。
- **集成与端到端**：要求提供 CI 日志或截图证明接口、消息链路、数据库等集成测试通过；LLM 核对接口契约与测试用例是否一致。

### 3. 文档与交付物（25%）
- **技术文档**：对照 ISO/IEC/IEEE 26514 清单确认架构图、部署流程、运维/故障排查指南是否齐备；LLM 输出缺口清单。
- **用户手册**：检查是否包含角色化操作步骤、可复现示例数据；LLM 复述操作流程评估逻辑完整性。
- **API 文档**：要求使用 OpenAPI/AsyncAPI 规范，并通过 Spectral/Postman Governance 等工具校验命名、错误码、示例一致性；违规项计入扣分。
- **资料一致性**：LLM 比对 README、架构说明、API 文档间的术语、版本号，标记矛盾项；反馈需补充的截图、示意图。

## AI/LLM 评审落地建议
- **数据准备**：将仓库代码、测试报告、文档等切分后通过向量检索或分层摘要提供给 LLM，控制上下文长度，保障不同项目公平。
- **提示设计**：为各评分项编写统一 Prompt，要求 LLM 输出“证据路径 + 结论 + 置信度”；对低置信度条目自动触发人工复核。
- **可信度控制**：静态分析、覆盖率、API 治理工具得分作为硬约束；LLM 仅对描述性或跨文档一致性进行加减分，所有扣分必须带证据。
- **公平性策略**：跨语言统一采用复杂度、重复率、覆盖率等指标，并按代码行数或技术债人天归一化；必要时按项目规模设置修正系数。
- **反馈机制**：向参赛团队输出自动生成的“改进清单”，便于迭代，也方便评委复核。

## 实施步骤建议
1. 确认工具链（SonarQube、PIT、Spectral/Postman 等），准备模板仓库与 CI 流程脚本。
2. 定义评分 Rubric 与权重，编写 LLM 评审提示词与输出模板。
3. 使用往届或样例项目试跑，校准分布与阈值，并确定硬性淘汰条件。
4. 赛前向参赛团队发布《质量与文档提交规范》，明确提交格式、上传路径与截止时间。
