# AI Code Review Judgement 自动评审方案

## 评估流程总览

## 评分流程概述
- 参赛项目提交的代码、文档、测试等材料统一接入“AI Code Review Judgement”流程。
- 系统依据首版 checklist 的 11 个检查项自动核对并打分，得到 100 分制的静态总分（代码质量 + 测试验证 + 文档交付），作为黑客松第一轮横向评估结果。
- 建议采用“基础度量 → AI 智能评审”的双阶段流程：先由各语言常见的静态检查/测试工具生成硬指标，再由 LLM 汇总证据、给出解释型评分。整体方案可作为“AI Code Review Judgement”自动评审系统落地实施。
- 方案面向 2 周黑客松，假设参赛者只能利用业务时间打磨作品，因此仅保留大多数团队可快速达成的基础标准，所有工具链均为成熟且易获取的开源或 SaaS 服务。
- 本项目聚焦于构建“AI Code Review Judgement”自动评审方案，交付目标是端到端的 AI 自动评审流程。
- 本次项目主要覆盖 TypeScript/JavaScript、Java、Python 与 Golang 技术栈；评分 Rubric 按语言推荐默认工具，确保参赛者以最少配置完成评审要求。
- 初始权重建议为“代码质量 40% + 测试验证 35% + 文档与交付 25%”，后续可结合赛况微调；若某一部分缺失，可按比例从其他部分扣减。

## 评分维度细则

### 1. 代码质量（40%）
- **基础静态检查**：提交一次成功的静态检查/格式化流水线输出。推荐工具：TypeScript/JavaScript 使用 ESLint + Prettier，Java 使用 Checkstyle 或 SpotBugs，Python 使用 Flake8 或 Ruff，Golang 使用 golangci-lint。评审关注规则是否启用基础语法/风格/常见缺陷检测，代码需通过检查。
- **编译与依赖健康**：提供最近一次构建或打包成功的日志，证明主分支可编译/启动。同步运行依赖安全扫描（如 `npm audit`、`pip-audit`、`mvn org.owasp:dependency-check`、`osv-scanner`、`go list -m all | osv-scanner`），仅要求列出高风险问题并给出处理计划。
- **核心代码说明**：提交简短的模块说明（可直接写在 README 或代码注释），指出关键目录与主要服务入口，便于 LLM 自动定位逻辑。无需提供详细架构图。

### 2. 测试验证（35%）
- **基础自动化测试**：提供至少一类自动化测试（单元或组件测试）并确保在 CI 中运行。语句覆盖率达到 60% 即可；如暂无法产出覆盖率报告，可提交核心功能的测试清单与运行截图作为替代。
- **关键路径演示**：准备最少一份接口或端到端冒烟脚本（如 Postman Collection、pytest API、Go test、JUnit + RestAssured），覆盖用户最常见的 2–3 个场景，生成可复现的执行证据。
- **结果记录**：提交 CI 流水线链接或执行日志压缩包，确保能够查验测试通过情况；无需额外撰写正式测试计划与缺陷报告。

### 3. 文档与交付（25%）
- **README 快速上手**：README 至少包含项目概述、依赖安装、启动命令、测试命令、默认账号或示例数据。结构可参照常见开源项目模板。
- **配置与环境**：提供 `.env.example` 或配置说明，列出必须的环境变量、第三方服务与简易替代方案。
- **接口与使用说明**：若项目提供 API、CLI 或前端页面，附简短说明表（无需完整 OpenAPI）：列出路径/命令、主要参数、示例响应或截图。文档以文字即可，无需额外工具校验。

## AI/LLM 评审落地建议
- **数据准备**：收集静态检查结果、构建与测试日志、README 等内容，直接打包为结构化文件（如 JSON 或 Markdown）。在 Gemini 2.5 Pro 等高上下文模型可容纳海量输入的情况下，默认直接整理材料喂给 LLM 即可；仅当上下文受限或需要跨项目复用结果时，再考虑引入语义检索等扩展能力。
- **提示设计**：针对三大评分维度编写固定 Prompt，要求 LLM 输出“结论 + 证据片段 + 置信度”。对置信度低的项目进行高亮标记即可，无需额外复核环节。
- **可信度控制**：静态检查与测试命令的返回状态作为硬约束；LLM 主要负责解释得分、指出缺口。若缺少硬指标，则在最终报告中标记“信息不足”。
- **公平性策略**：为四种语言提供默认命令模版与容器镜像（例如 Node.js + ESLint、Maven + SpotBugs、Python + pytest、Go + golangci-lint），保持评审标准一致。可根据仓库规模自动归一化代码行数后计算得分。
- **反馈机制**：自动生成一页式报告，列出各维度得分、关键证据及建议的快速修复项，便于团队在剩余时间内迭代。

## 自动评审打分清单（第一稿）
- Checklist 贴合“AI Code Review Judgement”自动评审系统的第一轮横向评分，合计 100 分，对应“代码质量 40 分 + 测试验证 35 分 + 文档交付 25 分”。
- 默认按照“未满足记 0 分、部分满足记 50% 分、完全满足记满分”的原则评分，特殊情况由 LLM 判定具体扣分幅度。
- Checklist 可在后续迭代中扩展更多检查点或调整分值，建议每次增改后保留总代分为 100 分，方便横向比较。

| 维度 | 检查项 | 判定标准 | 分值 |
| --- | --- | --- | --- |
| 代码质量 | 静态检查通过 | Lint/静态分析流水线覆盖核心规则并成功通过，附最新运行日志或报告 | 15 |
| 代码质量 | 构建/打包成功 | 提供最近一次构建或打包成功的日志或 CI 记录 | 10 |
| 代码质量 | 依赖安全扫描 | 执行依赖安全扫描，列出高风险项及处理计划 | 8 |
| 代码质量 | 核心模块说明 | README 或代码注释清晰标注关键目录、服务入口与业务描述 | 7 |
| 测试验证 | 自动化测试执行 | 至少一种自动化测试在 CI 中执行并通过 | 15 |
| 测试验证 | 覆盖率或核心用例证明 | 覆盖率 ≥ 60%；若无法生成覆盖率，提供核心用例清单与运行截图 | 10 |
| 测试验证 | 冒烟/关键路径脚本 | 提供覆盖 2–3 个典型场景的接口或端到端冒烟脚本及结果 | 6 |
| 测试验证 | 结果记录完整 | 提供 CI 流水线链接或执行日志压缩包，证明测试成功 | 4 |
| 文档交付 | README 快速上手 | README 包含项目概述、依赖安装、启动/测试命令、示例数据 | 12 |
| 文档交付 | 配置与环境说明 | 提供 `.env.example` 或配置说明，列出必要环境变量及替代方案 | 7 |
| 文档交付 | 接口/使用说明 | 列出主要 API、CLI 或页面入口，附示例参数/响应/截图 | 6 |

## 实施步骤建议
1. 提供预配置的 CI/CLI 脚本，封装 `lint`、`test`、`coverage`、`audit` 等常用命令，参赛者 Fork 后只需选择语言即可运行。
2. 准备 LLM 评审 Prompt 与示例输出，将静态检查、测试日志、README 等打包传入，即可得到结构化评分。
3. 使用历史或样例项目进行干跑，验证评分区间与报告模版，可在首轮提交后微调权重或判定阈值。
4. 赛前发布《质量自检清单》与模版仓库，明确提交材料、CI 通过标准与评审时间线，确保参赛者重点投入可达成的基础指标。
5. 在现成工具链基础上，由 2-3 名工程师投入约 5 个工作日即可完成“AI Code Review Judgement”系统的最小可用版本；若于 2025 年 9 月 26 日启动建设，可将上线目标设在 2025 年 10 月 3 日。

## 项目目标与意义
- 面向黑客松场景提供全自动、可解释的代码质量评审，实现快速横向筛选。
- 通过标准化的 checklist 与评分模型，让多语言项目在统一框架下得到可比较的分数。
- 为后续自动化扩展（如性能评估、可观测性分析等）奠定统一的数据与流程基础。
