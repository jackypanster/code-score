# AI Code Review Judgement MVP 功能说明

## 产品定位
- 目标：在 2 周黑客松赛程内，为参赛项目提供 100% 自动化、可解释的静态质量评分。
- 版本策略：以 MVP 形态先跑通端到端流程，后续迭代再扩展高级度量与人工协同能力。

## 用户与触发方式
- **参赛团队**：通过模板仓库/CI 提交代码，触发自动评审任务。
- **赛事运营方**：在管理面板或 CLI 中查看评分报告、下载得分数据。

## 核心能力拆解
1. **材料收集**
   - 支持指定仓库地址与提交哈希。
   - 自动下载 README、测试日志、CI 产物；若未提供测试日志，则标记 checklist 对应项为 0 分。
2. **基础度量执行**
   - 按语言调用预置脚本：`lint`, `test`, `coverage`, `audit`。
   - 输出结构化 JSON，字段覆盖 checklist 所需的所有判定标准。
3. **Checklist 映射**
   - 11 个检查项由规则引擎先行打标签（满足/部分/不满足）。
   - 对于“部分满足”情形，提供可读判断依据（例如覆盖率 42% → 部分满足）。
4. **LLM 打分**
   - Prompt 输入：项目信息、各项基础指标、证据片段。
   - LLM 输出：每项 0/部分/满分与置信度、引用证据；系统换算为数值分。
5. **报告生成**
   - 产出 `scorecard.json`（机器可读）与 `report.md`（人类可读）。
   - 包含总分、分项得分、主要不足、置信度提示。
6. **结果分发**
   - 支持写入对象存储、发送 Webhook、或生成 CSV 汇总。
   - 提供基础健康状态（成功/失败、耗时）。

## MVP 接口定义
- `POST /jobs`：提交评审任务，参数包括 `repo_url`, `commit_sha`, `language`, `artifact_urls`。
- `GET /jobs/{id}`：查询任务状态与结果下载链接。
- `GET /reports/{id}`：获取最终 `scorecard.json` 与 `report.md`。
（注：MVP 可用 CLI 命令替代 API，接口定义为后续演进目标。）

## Checklist 分值映射
- 代码质量：15 + 10 + 8 + 7 = 40 分。
- 测试验证：15 + 10 + 6 + 4 = 35 分。
- 文档交付：12 + 7 + 6 = 25 分。
- 判定规则：
  - 满足 → 100% 分值；
  - 部分满足 → 50% 分值（可按实际规则调整为区间，如 30%-70%）；
  - 未满足 → 0 分。
- LLM 负责根据证据决定类别，系统负责换算分值并生成总分。

## MVP 完成标准
- 支持四种语言（TS/JS、Java、Python、Go）至少各 1 个样例项目成功跑通。
- 全流程自动化完成，无人工介入。
- 生成的报告包含总分、分项得分、证据摘要、置信度。
- CLI 或 API 返回处理耗时、错误信息，并在失败时提供可追踪日志。

## 迭代路线（概述）
1. **M1（当前）**：跑通基础度量 + LLM 打分，输出报告。
2. **M2**：优化评分 granularity（区间分）、增加更多指标（性能、可观测性）。
3. **M3**：引入评委工作台、人工复核协同、历史对比分析。
