# Feature Specification: Checklist Mapping & Scoring Input MVP

**Feature Branch**: `002-git-log-docs`
**Created**: 2025-09-27
**Status**: Draft
**Input**: User description: "é€šè¿‡git logå’Œé¡¹ç›®æ–‡æ¡£ @docs/ å›é¡¾æˆ‘ä»¬ç¬¬ä¸€æ­¥å·²ç»è½åœ°çš„å·¥ä½œï¼Œç„¶åå¼€å§‹ç¬¬äºŒæ­¥ï¼šç¬¬äºŒæ­¥è§„åˆ’ï¼šChecklist æ˜ å°„ä¸è¯„åˆ†è¾“å…¥ MVP

  - ç›®æ ‡
    åŸºäºç¬¬ä¸€æ­¥ç”Ÿæˆçš„åŸå§‹åº¦é‡ç»“æœï¼Œè½åœ°"è‡ªåŠ¨è¯„å®¡æ‰“åˆ†æ¸…å•ï¼ˆç¬¬ä¸€ç¨¿ï¼‰"çš„åˆ¤å®šé€»è¾‘ï¼Œäº§å‡ºç»“æ„åŒ– score_input.jsonï¼Œä¾›åç»­ LLM è¯„åˆ†æˆ–ç›´æ¥ç”Ÿæˆé¦–ç‰ˆåˆ†å€¼ã€‚è¿™éƒ¨åˆ†å·¥ä½œèšç„¦åœ¨ docs/ai-code-review-judgement.md çš„ 11 ä¸ªæ£€æŸ¥é¡¹å’ŒåŠŸèƒ½è§„æ ¼é‡Œçš„"Checklist æ˜ å°„"é˜¶æ®µã€‚
  - æ‹†è§£ä»»åŠ¡
      1. è§„åˆ™å¼•æ“éª¨æ¶ï¼šæ–°å¢ ChecklistEvaluator æ¨¡å—ï¼Œè¯»å– submission.json ä¸­çš„ lint/test/audit/README ç­‰æŒ‡æ ‡ï¼Œé€é¡¹äº§å‡º status âˆˆ {met, partial, unmet} å’ŒåŸå§‹è¯æ®å¼•ç”¨ã€‚
      2. æ•°å€¼æ¢ç®—ä¸è¾“å‡ºæ ¼å¼ï¼šæ ¹æ®æ–‡æ¡£è®¾å®šçš„åˆ†å€¼ï¼ˆ15/10/...ï¼‰ï¼ŒæŠŠçŠ¶æ€æ˜ å°„ä¸ºåˆ†æ•°ï¼ˆ100%ã€50%ã€0%ï¼‰ï¼Œç”Ÿæˆç»Ÿä¸€çš„ score_input.jsonï¼ˆå« 11 æ¡è®°å½•ã€æ€»åˆ†å ä½ã€è¯æ®è·¯å¾„ï¼‰ï¼Œå¹¶å°† Markdown æ‘˜è¦å†™å…¥ output/report.md å°¾éƒ¨ï¼Œæ–¹ä¾¿äººå·¥éªŒè¯ã€‚
      3. å›å½’éªŒè¯ï¼šä»¥ AIGCInnovatorSpace/code-walker ä¸ºç¤ºä¾‹è¿è¡Œï¼Œç¡®è®¤å„æ£€æŸ¥é¡¹éƒ½æœ‰åˆ¤å®šç»“æœï¼›å†æŒ‘é€‰è‡³å°‘ä¸€ä¸ªä¸åŒè¯­è¨€çš„å…¬å¼€ä»“åº“å¿«é€Ÿè·‘ä¸€éï¼ŒéªŒè¯å¤šè¯­è¨€åŸºç¡€æŒ‡æ ‡å¯æ˜ å°„ã€‚
      4. è‡ªåŠ¨åŒ–æµ‹è¯•ï¼šç¼–å†™æœ€å°å•å…ƒ/é›†æˆæµ‹è¯•ï¼Œè¦†ç›–å…¸å‹æƒ…å†µä¸"éƒ¨åˆ†æ»¡è¶³"åˆ†æ”¯ï¼Œç¡®ä¿è§„åˆ™å¯ç»´æŠ¤ã€æ–¹ä¾¿è¿­ä»£ã€‚
  - è¾“å‡ºç‰©
      - æ–°çš„ score_input.json æ–‡ä»¶ï¼ˆæœºå™¨å¯è¯»ï¼‰
      - æ›´æ–°çš„ Markdown æŠ¥å‘ŠèŠ‚ï¼ˆäººå·¥æ£€è§†ï¼‰
      - ç›¸å…³å®ç°æ–‡æ¡£/README è¡¥å……ä½¿ç”¨è¯´æ˜ä¸æ‰©å±•æŒ‡å—

  å®Œæˆåï¼Œç¬¬ä¸€ä¸ª MVP é˜¶æ®µå°±æ‹¥æœ‰"ææ–™â†’åŸºç¡€åº¦é‡â†’Checklist å¾—åˆ†è¾“å…¥"çš„é—­ç¯ï¼Œä¸ºç¬¬ä¸‰æ­¥ï¼ˆè°ƒç”¨ LLM ç”Ÿæˆè§£é‡Šå‹æŠ¥å‘Šï¼‰æ‰“åŸºç¡€ã€‚"

## Execution Flow (main)
```
1. Parse user description from Input
   â†’ Identified: checklist evaluation system building on existing metrics collection
2. Extract key concepts from description
   â†’ Actors: hackathon organizers, repository analyzers, LLM scoring system
   â†’ Actions: evaluate checklist items, map metrics to scores, generate structured output
   â†’ Data: submission.json metrics, 11-item checklist, score_input.json output
   â†’ Constraints: 100-point scoring system, three-tier status mapping (met/partial/unmet)
3. For each unclear aspect:
   â†’ Score calculation algorithms defined in ai-code-review-judgement.md
4. Fill User Scenarios & Testing section
   â†’ Clear user flow: metrics â†’ evaluation â†’ structured scoring input
5. Generate Functional Requirements
   â†’ All requirements testable against existing checklist criteria
6. Identify Key Entities (scoring data models)
7. Run Review Checklist
   â†’ No implementation details, focused on evaluation logic and output format
8. Return: SUCCESS (spec ready for planning)
```

---

## âš¡ Quick Guidelines
- âœ… Focus on WHAT evaluation logic is needed and WHY
- âŒ Avoid HOW to implement (no specific code architecture, class names)
- ğŸ‘¥ Written for stakeholders who need automated code quality evaluation

### Section Requirements
- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn't apply, remove it entirely (don't leave as "N/A")

### For AI Generation
When creating this spec from a user prompt:
1. **Mark all ambiguities**: Use [NEEDS CLARIFICATION: specific question] for any assumption you'd need to make
2. **Don't guess**: If the prompt doesn't specify something (e.g., "login system" without auth method), mark it
3. **Think like a tester**: Every vague requirement should fail the "testable and unambiguous" checklist item
4. **Common underspecified areas**:
   - User types and permissions
   - Data retention/deletion policies  
   - Performance targets and scale
   - Error handling behaviors
   - Integration requirements
   - Security/compliance needs

---

## User Scenarios & Testing *(mandatory)*

### Primary User Story
As a hackathon organizer using the automated code review system, I need to convert raw repository metrics into structured scoring input that can be processed by LLM evaluation or direct scoring algorithms, so that I can fairly and consistently evaluate multiple participant repositories according to the 11-item quality checklist.

### Acceptance Scenarios
1. **Given** a `submission.json` file with repository metrics from the first phase, **When** the checklist evaluator processes the metrics, **Then** the system generates a `score_input.json` file with status evaluation (met/partial/unmet) for all 11 checklist items
2. **Given** metrics indicating successful linting and build completion, **When** the evaluator maps these to checklist items, **Then** code quality items receive appropriate status scores with evidence references
3. **Given** incomplete or missing test coverage data, **When** the evaluator processes testing metrics, **Then** affected checklist items are marked as "partial" or "unmet" with clear reasoning
4. **Given** a multi-language repository analysis, **When** the system processes language-specific metrics, **Then** the scoring logic adapts appropriately while maintaining consistent evaluation criteria

### Edge Cases
- What happens when submission.json contains null or missing metric sections?
- How does the system handle repositories with no README or documentation?
- What occurs when security audit tools find zero vulnerabilities vs. tools not being available?
- How does the evaluator distinguish between test failures and tests not being run?

## Requirements *(mandatory)*

### Functional Requirements
- **FR-001**: System MUST read metrics data from existing `submission.json` format generated by the first phase
- **FR-002**: System MUST evaluate each of the 11 checklist items defined in `ai-code-review-judgement.md` against available metrics
- **FR-003**: System MUST assign status values of "met", "partial", or "unmet" to each checklist item based on evaluation logic
- **FR-004**: System MUST generate structured `score_input.json` output containing all 11 checklist evaluations with evidence references
- **FR-005**: System MUST apply the defined point mapping (15/10/8/7/6/4 points) from the checklist specification
- **FR-006**: System MUST include evidence paths and reasoning for each checklist item evaluation
- **FR-007**: System MUST append evaluation summary to `output/report.md` in human-readable format for verification
- **FR-008**: System MUST handle missing or incomplete metrics gracefully without failing the entire evaluation
- **FR-009**: System MUST support multi-language repository evaluation using language-appropriate metric mappings
- **FR-010**: System MUST validate against known test repositories (code-walker and additional multi-language examples)
- **FR-011**: System MUST provide total score calculation and percentage breakdown by evaluation category
- **FR-012**: System MUST maintain evaluation logic that can be extended or modified for future checklist iterations

### Key Entities *(include if feature involves data)*
- **ChecklistItem**: Represents one of 11 evaluation criteria with dimension (code quality/testing/documentation), point value, evaluation status, and evidence references
- **EvaluationResult**: Container for complete checklist evaluation including item-level results, total score, category breakdowns, and execution metadata
- **ScoreInput**: Structured output format containing evaluation results, evidence paths, and metadata for downstream LLM processing or direct scoring
- **MetricsMapping**: Logic for translating raw submission.json metrics into checklist item evaluations, handling language-specific variations and missing data scenarios
- **EvidenceReference**: Specific citations linking checklist evaluations back to source metrics data, file paths, or tool outputs for auditability

---

## Review & Acceptance Checklist
*GATE: Automated checks run during main() execution*

### Content Quality
- [x] No implementation details (languages, frameworks, APIs)
- [x] Focused on user value and business needs
- [x] Written for non-technical stakeholders
- [x] All mandatory sections completed

### Requirement Completeness
- [x] No [NEEDS CLARIFICATION] markers remain
- [x] Requirements are testable and unambiguous
- [x] Success criteria are measurable
- [x] Scope is clearly bounded
- [x] Dependencies and assumptions identified

---

## Execution Status
*Updated by main() during processing*

- [x] User description parsed
- [x] Key concepts extracted
- [x] Ambiguities marked
- [x] User scenarios defined
- [x] Requirements generated
- [x] Entities identified
- [x] Review checklist passed

---
