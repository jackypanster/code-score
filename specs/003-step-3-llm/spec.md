# Feature Specification: LLM-Generated Code Review Reports

**Feature Branch**: `003-step-3-llm`
**Created**: 2025-09-27
**Status**: Draft
**Input**: User description: "Step 3ï¼šLLM è¯„è¯­ç”Ÿæˆï¼ˆæœ€å°å¯è¡Œå¢é‡ï¼‰

  ç›®æ ‡ï¼šåœ¨å·²ç»è·‘é€šçš„"åŸºç¡€åº¦é‡ + Checklist AI æ‰“åˆ†"åŸºç¡€ä¸Šï¼Œæ–°å¢ä¸€ä¸ªè½»é‡çš„ LLM æŠ¥å‘Šç”Ÿæˆæ­¥éª¤ï¼Œè®©è¯„å®¡åªéœ€ä¸€æ¡å‘½ä»¤å°±èƒ½å¾—åˆ°æœ€ç»ˆå¯è¯»çš„è¯´æ˜æ–‡æ¡£ï¼ŒåŒæ—¶ä¿æŒæ”¹åŠ¨èŒƒå›´æå°ã€æ˜“äºè¿­ä»£ã€‚

  â€”â€”â€”

  ### 3.A Prompt è®¾è®¡ï¼ˆT025ï¼‰

  - åˆ›å»º specs/prompts/llm_report.mdï¼Œå†™æˆæ¨¡æ¿ï¼ˆMarkdownï¼‰ï¼ŒåŒ…å«ä»¥ä¸‹å ä½å­—æ®µï¼š
      - é¡¹ç›®ä¿¡æ¯ï¼šä»“åº“ URLã€è¯­è¨€ã€åˆ†ææ—¶é—´ã€‚
      - Checklist æ€»åˆ†ä¸ç±»åˆ«åˆ†ï¼ˆä»£ç è´¨é‡ / æµ‹è¯• / æ–‡æ¡£ï¼‰ã€‚
      - æ¯ä¸ªçŠ¶æ€åˆ†ç»„çš„æ‘˜è¦ï¼ˆâœ… met, âš ï¸ partial, âŒ unmetï¼‰ã€‚
      - å…³é”®è¯æ®ï¼ˆå¯é™åˆ¶æ¡æ•°ï¼Œåªå–é«˜ç½®ä¿¡åº¦æˆ– Top Nï¼‰ã€‚
      - Warnings / pipeline æç¤ºã€‚
  - æ¨¡æ¿ç¤ºä¾‹ç‰‡æ®µï¼ˆå ä½ç¬¦åŒèŠ±æ‹¬å·ï¼‰ï¼š

    # AI Code Review Judgement Summary

    - Repository: {{repository.url}}
    - Commit: {{repository.commit_sha}}
    - Language: {{repository.primary_language}}
    - Evaluation Date: {{evaluation.timestamp}}

    ## Score Overview
    - Total Score: {{total.score}} / 100 ({{total.percentage}}%)

    ## Highlights
    {{#each met_items}}
    - âœ… {{name}} â€” {{description}}
    {{/each}}

    ## Improvement Opportunities
    {{#each unmet_items}}
    - âŒ {{name}} â€” {{description}}
    {{/each}}
  - åœ¨æ–‡æ¡£å†…è¯´æ˜ï¼šPrompt æ¶ˆè´¹ score_input.json çš„å“ªäº›å­—æ®µã€å¦‚ä½•è£å‰ªå†…å®¹ï¼ˆä¾‹å¦‚åªå¼• 3 æ¡è¯æ®ã€é˜²æ­¢è¶…é•¿ï¼‰ã€‚

  â€”â€”â€”

  ### 3.B LLM Runner æ¨¡å—ï¼ˆT026ï¼‰

  - æ–°å»º src/llm/report_generator.pyï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒæ–¹æ³•ï¼š
      1. load_score_input(path): è¯»å– score_input.jsonï¼ˆå¯å¤ç”¨ Pydantic æ¨¡å‹ï¼‰ã€‚
      2. build_prompt(score_input, prompt_template): æŒ‰æ¨¡æ¿å¡«å……æ•°æ®ï¼ˆå¯ç”¨ç®€å•çš„ .format()ã€Jinja2 æˆ–æ‰‹å†™æ›¿æ¢ï¼‰ã€‚
          - å¿…è¦æ—¶è£å‰ªæ•°æ®ï¼šä¾‹å¦‚åªé€‰å‰ 3 æ¡ evidence_summaryï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ã€‚
      3. run_gemini(prompt, output_path): è°ƒç”¨ Gemini CLIï¼š

         cmd = [
             \"gemini\",
             \"--approval-mode\", \"yolo\",
             \"-m\", \"gemini-2.5-pro\",
             \"--debug\",
             prompt
         ]
         subprocess.run(cmd, check=True, text=True, capture_output=not verbose)
          - å°† LLM è¾“å‡ºå†™å…¥ output/final_report.mdï¼ˆæˆ–ç”±è°ƒç”¨è€…æŒ‡å®šï¼‰ã€‚
      4. generate_report(score_input_path, output_dir, prompt_path)ï¼šç»¼åˆä¸Šè¿°æ­¥éª¤ï¼Œå®Œæˆæ•´ä¸ªæµç¨‹ã€‚
  - é”™è¯¯å¤„ç†ä¸å¯æ‰©å±•æ€§ï¼š
      - æ•æ‰ subprocess.CalledProcessErrorï¼Œæ‰“å° Gemini CLI çš„ stderr ä»¥ä¾¿è°ƒè¯•ã€‚
      - ç•™å‡º"è‡ªå®šä¹‰å‘½ä»¤/æ¨¡å‹"å‚æ•°ï¼Œæ–¹ä¾¿ä»¥ååˆ‡æ¢ï¼ˆOpenRouterã€Claude CLI ç­‰ï¼‰ã€‚

  â€”â€”â€”

  ### 3.C CLI é›†æˆä¸æµ‹è¯•ï¼ˆT027ï¼‰

  1. å‘½ä»¤è¡Œæ¥å£
      - åœ¨ src/cli/main.py æˆ–å•ç‹¬å‘½ä»¤æ–‡ä»¶ä¸­æ–°å¢å‘½ä»¤ï¼š

        code-score llm-report ./output/score_input.json \
          --prompt specs/prompts/llm_report.md \
          --output ./output/final_report.md
      - å¯¹åŸ code-score analyze å‘½ä»¤æ–°å¢å¯é€‰ flag --generate-llm-reportï¼Œç›´æ¥åœ¨åˆ†æç»“æŸåè°ƒç”¨ Runnerï¼ˆå…ˆç¡®è®¤ score_input.json å·²ç”Ÿäº§ï¼‰ã€‚
  2. å•å…ƒ/é›†æˆæµ‹è¯•
      - è®¾è®¡ä¸€ä¸ªå‡å“åº”çš„æµ‹è¯•ï¼šmock subprocess.runï¼ŒéªŒè¯ä¼ å…¥çš„å‘½ä»¤ã€Prompt æ˜¯å¦åŒ…å«é¢„æœŸå…³é”®ä¿¡æ¯ã€‚
      - åœ¨é›†æˆæµ‹è¯•é‡Œï¼Œæ„é€ ä¸€ä¸ªå° score_input.jsonï¼Œè¿è¡Œ Runnerï¼Œæ–­è¨€ç”Ÿæˆçš„ final_report.md å«æ€»åˆ†ã€ä¸»è¦é¡¹åç§°ç­‰ã€‚
  3. æ–‡æ¡£æ›´æ–° / ä½¿ç”¨æŒ‡å—
      - åœ¨ README æˆ– docs ä¸­æ–°å¢æ®µè½ï¼Œè¯´æ˜ï¼š
          - å¦‚ä½•æ‰‹åŠ¨è¿è¡Œ code-score llm-report â€¦
          - å¸¸è§å‚æ•°ï¼ˆå¦‚å…³é—­ç”Ÿæˆã€åˆ‡æ¢æ¨¡æ¿ã€åªæ‰“å° Promptï¼‰
          - Gemini CLI å‰ç½®å®‰è£…è¦æ±‚ã€‚

  â€”â€”â€”"

## Execution Flow (main)
```
1. Parse user description from Input
   â†’ Feature: Add LLM-generated report step to existing metrics + checklist pipeline
2. Extract key concepts from description
   â†’ Actors: Code reviewers, evaluators, hackathon judges
   â†’ Actions: Generate human-readable reports from structured evaluation data
   â†’ Data: score_input.json from checklist evaluation, prompt templates, final reports
   â†’ Constraints: Minimal code changes, iterative approach, single command execution
3. For each unclear aspect:
   â†’ [NEEDS CLARIFICATION: Which LLM providers beyond Gemini should be supported?]
   â†’ [NEEDS CLARIFICATION: Should reports be customizable for different evaluation contexts?]
4. Fill User Scenarios & Testing section
   â†’ Primary flow: Reviewer runs single command to get final readable report
5. Generate Functional Requirements
   â†’ Template-based report generation, LLM integration, CLI extensions
6. Identify Key Entities
   â†’ Report Template, LLM Provider Configuration, Generated Report
7. Run Review Checklist
   â†’ No implementation details exposed to business stakeholders
8. Return: SUCCESS (spec ready for planning)
```

---

## âš¡ Quick Guidelines
- âœ… Focus on WHAT users need and WHY
- âŒ Avoid HOW to implement (no tech stack, APIs, code structure)
- ğŸ‘¥ Written for business stakeholders, not developers

---

## User Scenarios & Testing

### Primary User Story
As a code reviewer or hackathon judge, I want to generate a comprehensive, human-readable evaluation report from structured code quality data, so that I can quickly understand project strengths and areas for improvement without manually interpreting raw metrics.

### Acceptance Scenarios
1. **Given** I have completed code quality analysis and checklist evaluation for a repository, **When** I run the report generation command, **Then** I receive a well-formatted final report that summarizes scores, highlights key strengths, and identifies improvement opportunities
2. **Given** I have custom evaluation criteria or reporting needs, **When** I specify a custom prompt template, **Then** the system uses my template instead of the default one
3. **Given** I want to integrate report generation into my automated workflow, **When** I run the analysis command with the report generation flag, **Then** the system automatically produces both the evaluation data and the final report in a single execution

### Edge Cases
- What happens when the LLM service is unavailable or returns an error?
- How does the system handle very large evaluation datasets that might exceed LLM context limits?
- What occurs when required evaluation data is missing or incomplete?
- How does the system behave when custom prompt templates contain invalid placeholders?

## Requirements

### Functional Requirements
- **FR-001**: System MUST generate human-readable evaluation reports from structured checklist evaluation data
- **FR-002**: System MUST support template-based report customization with predefined placeholder fields
- **FR-003**: Users MUST be able to generate reports as a standalone command using existing evaluation data
- **FR-004**: System MUST integrate report generation as an optional step in the main analysis pipeline
- **FR-006**: System MUST truncate or summarize evaluation data to prevent LLM context overflow
- **FR-007**: System MUST handle LLM service errors gracefully and provide meaningful error messages
- **FR-008**: System MUST support [NEEDS CLARIFICATION: multiple LLM providers - should this be limited to Gemini or include others like OpenAI, Claude, etc.?]
- **FR-009**: Generated reports MUST include project metadata, score summaries, key findings, and improvement recommendations
- **FR-010**: System MUST validate that required evaluation data exists before attempting report generation
- **FR-011**: Users MUST be able to specify custom output paths for generated reports
- **FR-012**: System MUST [NEEDS CLARIFICATION: what level of report customization should be supported - just templates or also output format options?]

### Key Entities
- **Report Template**: Markdown-format template with placeholder fields for dynamic content injection, including project metadata, scores, evidence summaries, and recommendations
- **LLM Provider Configuration**: Settings and parameters for connecting to external language models, including model selection, API endpoints, and request formatting
- **Generated Report**: Final human-readable document containing evaluation summary, project assessment, and actionable recommendations based on code quality analysis

---

## Review & Acceptance Checklist
*GATE: Automated checks run during main() execution*

### Content Quality
- [x] No implementation details (languages, frameworks, APIs)
- [x] Focused on user value and business needs
- [x] Written for non-technical stakeholders
- [x] All mandatory sections completed

### Requirement Completeness
- [ ] No [NEEDS CLARIFICATION] markers remain
- [x] Requirements are testable and unambiguous
- [x] Success criteria are measurable
- [x] Scope is clearly bounded
- [x] Dependencies and assumptions identified

---

## Execution Status
*Updated by main() during processing*

- [x] User description parsed
- [x] Key concepts extracted
- [x] Ambiguities marked
- [x] User scenarios defined
- [x] Requirements generated
- [x] Entities identified
- [ ] Review checklist passed

---
