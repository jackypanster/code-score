"""
File validator utility for smoke test suite.

This module handles validation of output files generated by the pipeline,
including existence checks, content validation, and metadata collection.
"""

import json
import logging
from pathlib import Path
from typing import Any

from .models import OutputArtifact, ValidationResult

# Configure logging
logger = logging.getLogger(__name__)


class FileValidationError(Exception):
    """Raised when file validation fails."""
    pass


def validate_output_files(output_directory: Path) -> ValidationResult:
    """
    Validate all expected output files from the pipeline.

    Args:
        output_directory: Directory containing pipeline outputs

    Returns:
        ValidationResult with comprehensive validation status
    """
    expected_files = [
        "submission.json",
        "score_input.json",
        "evaluation_report.md"
    ]

    logger.info(f"Validating output files in: {output_directory}")

    # Validate each expected file
    output_artifacts = []
    for filename in expected_files:
        file_path = output_directory / filename
        artifact = validate_single_file(file_path, filename)
        output_artifacts.append(artifact)

    # Determine overall validation status
    all_files_present = all(artifact.file_exists for artifact in output_artifacts)
    all_content_valid = all(artifact.content_valid for artifact in output_artifacts)

    if all_files_present and all_content_valid:
        return ValidationResult.success(
            execution_summary=f"All {len(output_artifacts)} output files validated successfully",
            output_files=output_artifacts,
            pipeline_duration=0.0  # Will be updated by caller
        )
    else:
        # Generate detailed error message
        missing_files = [a.expected_name for a in output_artifacts if not a.file_exists]
        invalid_files = [a.expected_name for a in output_artifacts if a.file_exists and not a.content_valid]

        error_parts = []
        if missing_files:
            error_parts.append(f"Missing files: {', '.join(missing_files)}")
        if invalid_files:
            error_parts.append(f"Invalid content: {', '.join(invalid_files)}")

        error_message = "; ".join(error_parts)

        return ValidationResult.file_validation_failure(
            error_message=error_message,
            output_files=output_artifacts,
            pipeline_duration=0.0  # Will be updated by caller
        )


def validate_single_file(file_path: Path, expected_name: str) -> OutputArtifact:
    """
    Validate a single output file.

    Args:
        file_path: Path to the file to validate
        expected_name: Expected filename for validation

    Returns:
        OutputArtifact with validation results
    """
    logger.debug(f"Validating file: {file_path}")

    artifact = OutputArtifact(
        file_path=file_path,
        expected_name=expected_name
    )

    # The OutputArtifact.__post_init__ method will handle the validation
    # via the refresh() method call

    logger.debug(f"File validation result - exists: {artifact.file_exists}, "
                f"size: {artifact.file_size_bytes}, valid: {artifact.content_valid}")

    return artifact


def validate_json_schema(file_path: Path, schema_requirements: dict[str, Any] | None = None) -> bool:
    """
    Validate JSON file against basic schema requirements.

    Args:
        file_path: Path to JSON file
        schema_requirements: Optional schema requirements to check

    Returns:
        True if valid, False otherwise
    """
    try:
        with open(file_path) as f:
            data = json.load(f)

        # Basic validation: ensure it's a dict at the root level
        if not isinstance(data, dict):
            logger.warning(f"JSON file is not a dictionary: {file_path}")
            return False

        # Check schema requirements if provided
        if schema_requirements:
            return _validate_json_requirements(data, schema_requirements, file_path)

        return True

    except (OSError, json.JSONDecodeError) as e:
        logger.warning(f"JSON validation failed for {file_path}: {e}")
        return False


def _validate_json_requirements(
    data: dict[str, Any],
    requirements: dict[str, Any],
    file_path: Path
) -> bool:
    """Validate JSON data against schema requirements."""
    for key, expected_type in requirements.items():
        if key not in data:
            logger.warning(f"Required key '{key}' missing in {file_path}")
            return False

        if not isinstance(data[key], expected_type):
            logger.warning(f"Key '{key}' has wrong type in {file_path}: "
                          f"expected {expected_type}, got {type(data[key])}")
            return False

    return True


def validate_submission_json(file_path: Path) -> bool:
    """
    Validate submission.json file structure.

    Args:
        file_path: Path to submission.json

    Returns:
        True if valid, False otherwise
    """
    schema_requirements = {
        "schema_version": str,
        "repository": dict,
        "metrics": dict,
        "execution": dict
    }

    return validate_json_schema(file_path, schema_requirements)


def validate_score_input_json(file_path: Path) -> bool:
    """
    Validate score_input.json file structure.

    Args:
        file_path: Path to score_input.json

    Returns:
        True if valid, False otherwise
    """
    schema_requirements = {
        "evaluation_result": dict,
        "repository_info": dict
    }

    return validate_json_schema(file_path, schema_requirements)


def validate_markdown_file(file_path: Path) -> bool:
    """
    Validate Markdown file content.

    Args:
        file_path: Path to Markdown file

    Returns:
        True if valid, False otherwise
    """
    try:
        with open(file_path, encoding='utf-8') as f:
            content = f.read()

        # Basic validation checks
        if len(content.strip()) == 0:
            logger.warning(f"Markdown file is empty: {file_path}")
            return False

        # Check for basic Markdown structure (headers)
        if not any(line.startswith('#') for line in content.split('\n')):
            logger.warning(f"Markdown file appears to lack headers: {file_path}")
            return False

        # Note: We don't check isprintable() because valid Markdown contains newlines
        # which cause isprintable() to return False

        return True

    except (OSError, UnicodeDecodeError) as e:
        logger.warning(f"Markdown validation failed for {file_path}: {e}")
        return False


def get_output_file_summary(output_directory: Path) -> dict[str, Any]:
    """
    Get summary information about output files.

    Args:
        output_directory: Directory containing pipeline outputs

    Returns:
        Dictionary with summary information
    """
    expected_files = [
        "submission.json",
        "score_input.json",
        "evaluation_report.md"
    ]

    summary = {
        "output_directory": str(output_directory),
        "directory_exists": output_directory.exists(),
        "files": {},
        "total_size_bytes": 0,
        "all_files_present": True,
        "all_files_valid": True
    }

    for filename in expected_files:
        file_path = output_directory / filename
        file_info = {
            "exists": file_path.exists(),
            "size_bytes": 0,
            "valid_content": False
        }

        if file_path.exists():
            try:
                stat = file_path.stat()
                file_info["size_bytes"] = stat.st_size
                file_info["modified"] = stat.st_mtime
                summary["total_size_bytes"] += stat.st_size

                # Validate content
                artifact = validate_single_file(file_path, filename)
                file_info["valid_content"] = artifact.content_valid

            except OSError as e:
                logger.warning(f"Failed to get file stats for {filename}: {e}")

        else:
            summary["all_files_present"] = False

        if not file_info["valid_content"]:
            summary["all_files_valid"] = False

        summary["files"][filename] = file_info

    return summary


def cleanup_invalid_output_files(output_directory: Path) -> list[str]:
    """
    Clean up invalid output files.

    Args:
        output_directory: Directory containing pipeline outputs

    Returns:
        List of files that were cleaned up
    """
    if not output_directory.exists():
        return []

    expected_files = [
        "submission.json",
        "score_input.json",
        "evaluation_report.md"
    ]

    cleaned_files = []

    for filename in expected_files:
        file_path = output_directory / filename
        if file_path.exists():
            artifact = validate_single_file(file_path, filename)
            if not artifact.content_valid:
                try:
                    file_path.unlink()
                    cleaned_files.append(filename)
                    logger.info(f"Cleaned up invalid file: {filename}")
                except OSError as e:
                    logger.warning(f"Failed to clean up {filename}: {e}")

    return cleaned_files
